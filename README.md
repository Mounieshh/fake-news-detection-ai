# Fake News Detection System

A comprehensive Python-based system for detecting fake news through text preprocessing, feature extraction, and data storage. The system processes text, images, and URLs to extract linguistic and statistical features that can be used for fake news classification.

## Features

- **Multi-input Processing**: Handles text, images, and URLs
- **Advanced Preprocessing**: Text cleaning, normalization, and validation
- **Feature Extraction**: Comprehensive feature extraction including:
  - Statistical features (word count, sentence length, etc.)
  - Linguistic features (syllable count, word length distribution)
  - Sentiment analysis
  - Readability metrics (Flesch Reading Ease, Flesch-Kincaid Grade)
  - Fake news indicators (emotional words, urgency words, exaggeration)
  - Part-of-Speech (POS) tagging
  - Named Entity Recognition (NER)
  - N-gram analysis
- **Data Storage**: Automatic saving of preprocessing results to JSON files
- **Organized Storage**: Results organized by input type (text/image/url)
- **Export Functionality**: CSV export for analysis
- **Interactive Mode**: User-friendly command-line interface

## Project Structure

```
fake-news/
├── src/
│   ├── __init__.py
│   ├── main.py                    # Main application entry point
│   ├── data_manager.py            # Data storage and management
│   ├── input_handlers/
│   │   ├── __init__.py
│   │   ├── text_handler.py        # Text input processing
│   │   ├── image_handler.py       # Image input processing
│   │   └── url_handler.py         # URL input processing
│   └── preprocessing/
│       ├── __init__.py
│       ├── preprocessing_pipeline.py  # Main preprocessing pipeline
│       ├── feature_extractor.py       # Feature extraction
│       ├── text_cleaner.py            # Text cleaning utilities
│       └── data_validator.py          # Input validation
├── data/                          # Generated data storage (auto-created)
│   ├── text/                      # Text processing results
│   ├── image/                     # Image processing results
│   └── url/                       # URL processing results
├── main.py                        # Root-level entry point
├── requirements.txt               # Python dependencies
├── fake-news-detection.ipynb     # Jupyter notebook for analysis
└── README.md                      # This file
```

## Installation

### Prerequisites

- Python 3.7 or higher
- pip (Python package installer)

### Setup Instructions

1. **Clone the repository**
   ```bash
   git clone <your-repository-url>
   cd fake-news
   ```

2. **Create a virtual environment (recommended)**
   ```bash
   python -m venv venv
   
   # On Windows
   venv\Scripts\activate
   
   # On macOS/Linux
   source venv/bin/activate
   ```

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

4. **Download NLTK data (optional but recommended)**
   ```bash
   python -c "import nltk; nltk.download('punkt'); nltk.download('averaged_perceptron_tagger'); nltk.download('wordnet')"
   ```

5. **Download spaCy model (optional but recommended)**
   ```bash
   python -m spacy download en_core_web_sm
   ```

## Usage

### Command Line Interface

Run the main application:
```bash
python src/main.py
```

Or use the root-level entry point:
```bash
python main.py
```

### Interactive Mode

The system provides an interactive menu with the following options:

1. **Text input** - Process text directly
2. **Image file path** - Process images for text extraction
3. **URL** - Process web pages for content extraction
4. **System status** - View system capabilities and data summary
5. **List saved results** - View all saved preprocessing results
6. **Load saved result** - Load and view a specific result
7. **Export results to CSV** - Export all results metadata
8. **Exit** - Close the application

### Programmatic Usage

```python
from src.main import FakeNewsDetector

# Initialize the detector
detector = FakeNewsDetector()

# Process text input
result = detector.process_input("Your text here", "text", save_result=True)

# List saved results
saved_results = detector.list_saved_results()

# Export to CSV
csv_path = detector.export_results_to_csv()
```

## Data Storage

The system automatically saves preprocessing results to JSON files in the `data/` directory:

- **File naming**: `{cleaned_text}_{hash}_{timestamp}.json`
- **Organization**: Files are organized by input type (text/image/url)
- **Content**: Each file contains:
  - Original and cleaned text
  - Extracted features
  - Processing metadata
  - Validation results
  - Summary statistics

### Example JSON Structure

```json
{
  "status": "success",
  "input_type": "text",
  "original_text": "Your input text...",
  "cleaned_text": "cleaned version...",
  "data": {
    "features": {
      "word_count": 25,
      "sentiment_polarity": 0.1,
      "fake_news_indicator_score": 0.05,
      ...
    },
    "metadata": {
      "processing_timestamp": "2024-01-01T12:00:00",
      "feature_count": 50
    }
  },
  "summary": {
    "total_features_extracted": 50,
    "processing_successful": true
  },
  "_metadata": {
    "saved_at": "2024-01-01T12:00:00",
    "filename": "example_abc123_20240101_120000.json"
  }
}
```

## Dependencies

Key dependencies include:

- **nltk**: Natural language processing
- **spacy**: Advanced NLP and named entity recognition
- **PIL/Pillow**: Image processing
- **beautifulsoup4**: Web scraping for URL processing
- **requests**: HTTP requests for URL processing
- **pandas**: Data manipulation (optional)
- **numpy**: Numerical operations (optional)

See `requirements.txt` for the complete list.

## Configuration

The preprocessing pipeline can be configured through the `PreprocessingPipeline` class:

```python
config = {
    'cleaning': {
        'remove_html': True,
        'remove_urls': True,
        'remove_stopwords': True,
        'lemmatize': True,
        # ... more options
    },
    'feature_extraction': {
        'extract_statistical': True,
        'extract_sentiment': True,
        'extract_fake_news_indicators': True,
        # ... more options
    }
}

pipeline = PreprocessingPipeline(config)
```

## Troubleshooting

### Common Issues

1. **NLTK Data Error**: If you encounter "File is not a zip file" error:
   ```bash
   python -c "import nltk; nltk.download('punkt', force=True)"
   ```

2. **Missing Dependencies**: Install missing packages:
   ```bash
   pip install -r requirements.txt
   ```

3. **spaCy Model Not Found**: Download the English model:
   ```bash
   python -m spacy download en_core_web_sm
   ```

4. **Permission Errors**: Ensure you have write permissions for the `data/` directory.

## Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add some amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## License

This project is licensed under the MIT License - see the LICENSE file for details.

## Acknowledgments

- NLTK and spaCy communities for excellent NLP libraries
- Contributors to the fake news detection research field
- Open source community for various supporting libraries

## Future Enhancements

- [ ] Machine learning model integration
- [ ] Real-time processing capabilities
- [ ] Web interface
- [ ] API endpoints
- [ ] Advanced visualization features
- [ ] Multi-language support
- [ ] Cloud storage integration
